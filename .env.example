# Model API Configuration
# Base URL for the model API (OpenAI-compatible endpoint)
# For Ollama: http://localhost:11434/v1
# For OpenAI: https://api.openai.com/v1
MODEL_API_BASE=http://localhost:11434/v1

# API key for authentication
# For Ollama: Use any non-empty string (e.g., "ollama" or "dummy-key")
# For OpenAI: Use your actual API key
MODEL_API_KEY=ollama

# Model names for generation and verification
# For Ollama: Use model names from 'ollama list' (e.g., gpt-oss:20b, qwen3:8b)
# For OpenAI: Use model names like gpt-4, gpt-3.5-turbo
GENERATION_MODEL_NAME=gpt-oss:20b
VERIFICATION_MODEL_NAME=qwen3:8b

# Optional: Rate limiting configuration
# Maximum number of API requests per minute
# For Ollama (local): Can be higher (e.g., 120) since there's no API quota
# For OpenAI: Respect your tier limits (e.g., 60 for tier 1)
MAX_REQUESTS_PER_MINUTE=120

# Maximum number of tokens to process per minute
# For Ollama (local): Can be higher since there's no token quota
# For OpenAI: Respect your tier limits (e.g., 90000 for tier 1)
MAX_TOKENS_PER_MINUTE=200000

# Logging configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Optional: Timeout for API requests (seconds)
API_TIMEOUT=60

# Optional: Maximum retry attempts for failed API calls
MAX_RETRY_ATTEMPTS=3

# Optional: Exponential backoff base for retries (seconds)
RETRY_BACKOFF_BASE=2

